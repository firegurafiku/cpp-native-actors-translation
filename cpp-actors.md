====
Нативные акторы: масштабируемая программная платформа
для распределённого гетерогенного вычислительного окружения
===

(Авторы)

Аннотация
---------

Написание параллельных программ — нетривиальная задача, особенно при
использовании низкоуровневых примитивов, таких как потоки и блокировки
потоков в системе с разделяемой памятью. Модель акторов заменяет
неявную коммуникацию в системе явной пересылкой сообщений, в полном
соответствии с парадигмой отсутствия разделяемого состояния. Она
применима как для параллелизации, так и для распределения вычислений,
но до сих пор не была реализована в нативных языках
программирования. В данной работе представлен вариант архитектуры
модели акторов применительно к языку C++, а также рассмотрена
реализация библиотеки, соответствующей представленной архитектуре, для
распределённого параллельного гетерогенного аппаратного
окружения. Разработанная библиотека прозрачно интегрирует в одну
систему компьютерные и видеопроцессоры и позволяет разрабатывать
эффективные масштабируемые параллельные программы. Она включает в себя
неблокирующий алгоритм почтового ящика с поддержкой обработки
сообщений при помощи паттерн-матчинга. Тщательные измерения показали
чрезвычайно малое потребление памяти в реалистичных сценариях
использования, в то время как производительность не только превосходит
существующие развитые реализации модели акторов для распространённых
виртуальных машин, но и не уступает по производительности
низкоуровневым библиотекам таким как OpenMPI, превосходя их по уровню
масштабируемости.

(Каталогизатор)
(Ключевые слова)

Введение
--------

Большинство современных программ выполняется в среде с несколькими
вычислительными устройствами. Многоядерные процессоры стали
неотъемлемой частью даже мобильных устройств. Поэтому главной
сложностью в разработке программ стала необходимость подходящим
образом разделять ресурсы для, обеспечения приемлемого уровня
производительности, эффективности выполнения, удовлетворения прочих
требований к ПО.

(...) Все указанные сценарии требуют использования параллельных
вычислений [12], некоторые также требуют использования
распределённых. В то же время, большая часть приложений до сих пор
пишется на популярных императивных языках [28].

Императивные языки программирования, такие как C, C++ или Java, не
оперируют параллельными концепциями. Они были разработаны ещё до того,
как началась «многоядерная революция» и, таким образом, изначально
предназначались для работы на одноядерных компьютерах. Поэтому позже
для них были разработаны библиотеки, позволяющие запускать несколько
*потоков выполнения* в рамках одного процесса. Однако, работа с
многопоточностью нетривиальна, особенно в среде с разделяемое памятью,
где одновременный доступ к общей памяти процесса может легко привести
к *состояниям гонки* потоков.  Производительность и масштабируемость
приложений при использовании ручной синхронизации сильно зависит от
стратегии её применения. Грубые блокировки, устанавливаемые на крупные
блоки кода, реализуются просто, но ведут к долгим очередям к ресурсу
и, таким образом, к проблемам с масштабируемостью. Уменьшение величины
критических секций при помощи более тщательного выбора мест установки
блокировок улучшает масштабируемость, однако также и сложность системы
и её подверженность ошибкам, например, из-за нарушения порядка
блокировок. Кроме того, корректность такой программы оказывается
практически невозможно проверить при помощи систематического
тестирования, так как воспроизводимость ошибок сильно зависит от
конкретных условий запуска [13].

Для решения этих проблем Гевиттом, Бишопом и Штайгером [16] был
предложен принципиально иной подход — модель акторов. Их формализм
описывает одновременно работающие сущности, действующие независимо
друг от друга, не имеющие общего разделяемого состояния и
взаимодействующие друг с другом при помощи асинхронной посылки
сообщений. Так как акторы самодостаточны и не используют разделяемых
ресурсов, гонки потоков становятся невозможны по
определению. Взаимодействие при помощи сообщений позволяет выполнять
прозрачное развёртывание системы, причём акторы могут быть запущены:

 * на одной машине, в этом случае можно говорить о многопоточной
   вычислительной среде;
 * на разных процессорах одной машины и обращаться к разным областям
   оперативной памяти, в этом случае можно говорить о гетерогенной
   вычислительной среде;
 * на разным машинах, объединённых в сеть, образующих распределённую
   вычислительную среду.

Существующие акторные языки, такие как Erlang [3], и фреймворки, такие
как Akka [30] или Kilim [25], привязаны к своим специфическим нишам
либо API какого-либо конкретного производителя (например, Casablanca
[20]). Поэтому одна из целей данной работы — сделать акторное
программирование доступным для сообщества С++-программистов, расширив
таким образом область его применения. Для этого мы с нуля
спроектировали и реализовали акторный фреймворк для C++.

Данная работа ставит перед собой три основных цели. Во-первых, она
представляет программную платформу для использования модели акторов в
C++. Платформа включает в себя легковесную неблокирующую прозрачную
сетевую систему передачи сообщений между акторами, реализованную без
переключений контекста. Во-вторых, она описывает подход для
прозрачной интеграции гетерогенных аппаратных компонентов при помощи
OpenCL при помощи выведения интерфейса сообщений из сигнатуры
ядра. В-третьих, она демонстрирует на практике, что указанный подход
практичен и достаточно эффективен для того, чтобы превзойти как
существующие развитые акторные языки и фреймворки, так и, в некоторых
сценариях использования и ключевых аспектах, низкоуровневые библиотеки
передачи сообщений, такие как OpenMPI.

Мы собрали ключевые принципы нашего фреймворка в разделе 2, остальные
разделы работы посвящены различным важным аспектам разработки. Раздел
3 содержит ссылки на родственные работы, раздел 4 описывает некоторые
детали принятых архитектурных решений. В разделе 5 приведена
экспериментальная оценка производительности, затем в разделе 6
обсуждаются то, какие уроки мы вынесли из разработки. Наконец, раздел
7 подводит итог всей работе.

2. Место для акторов
--------------------

В данном разделе мы выделим несколько аргументов в поддержку модели
акторов с точки зрения процесса программирования и производительности
полученной программы. Рассмотрим следующий класс (реализующий
хранилище ключ-значение):

    class KeyValueStore {
    public:
        void set ( Key k , Value v ) {
            // ...
        }

        Value get ( Key k ) const {
            return ...;
        }
	private:
		// ... implementation details
    };

Если требуется одновременный доступ к хранилищу из нескольких потоков,
реализация классов должна обеспечивать потокобезопасность
данных. Простое решение состоит в том, чтобы обернуть код обоих
методов в критические секции при помощи мутекса. Этот подход не
слишком хорошо масштабируется, главным образом из-за того, что
становится невозможным одновременный доступ к данным даже на
чтение. Более масштабируемым решением была бы реализация некоторого
протокола сериализации, например, на основе рекурсивных или
разделяемых мутексов.

Следующий код иллюстрирует создание актора в нашей программной
платформе:

    become (
      on ( atom ( " set " ) , arg_match )
        >> [=]( Key k , Value v ) { /*on ( atom ( " get " ) , arg_match )
        >> [=]( Key k ) {
        reply (...);
        }
    );

Для программирования акторов даже не требуется знать о примитивах
синхронизации. В тоже время, наша реализация акторов поддерживает
высокопараллельный доступ (см. раздел 5.3) к данным. В приведённом
примере запросы ключа обрабатываются последовательно без
дополнительной координации процессов, так как отсутствует межакторная
синхронизация. Для ещё большего уровня параллелизма акторы могут
распределять задачи на набор т.н. воркеров, Наш интерфейс передачи
сообщений использует т.н. атомы вместо имён методов для идентификации
конкретных операций. Использование имён атомов не приводят к сравнению
строк в рантайме так как их имена на этапе компиляции  конвертируются
в целочисленные значения при помощи хеш-функции. Приведённый пример
показывает предыдущий пример для вызывающей стороны:

    sync_send ( server , atom ( " get " ) , key ). then (
    [=]( Value val ) {
    cout << key << " = > " << val << endl ;
    }
    );

Наша платформа предоставляет прозрачное сетевое взаимодействие между
акторами. Для того, чтобы подчеркнуть её производительность, мы
реализовали алгоритм нахождения неподвижных точек множества
Мандельброта. Вычисления были распределены при помощи нашей библиотеки
(libcppa) для акторного программирования и при помощи низкоуровневой
оптимихированной библиотеки OpenMPI. Така как обе программы
использовали одну и ту же реализацию C++ (компилятор и библиотеки),
результаты измерения отражают влияния только лишь накладные расходы
используемой технологии параллелизации. Результат измерения должен
также показать стоимость высокоуровневых абстракций libcppa по
сравнению с низкоуровневыми примитивами OpenMPI.

(Рисунок 1.)

Рисунок 1 показывает время работы тестовых программ как функцию от
количества используемых узлов. В этом эксперименте использовался один
аппаратный хост, на котором были запущены 12 виртуальных
машин. OpenMPI выигрывает при количестве узлов от одного до трёх, при
дальнейшем увеличении количества узлов libcppa выходит вперёд, что
указывает на её лучшую масштабируемость по сравнению с конкурентом.

(Рисунок 2.)

Результаты второго эксперимента приведены на рис. 2. В нём мы
использовали три хоста, на каждом из которых были запущены 12
виртуальных машин, при этом сложность решаемой задачи была увеличена
примерно в три раза. Для сохранение равномерной загрузки
распределённой сети, узлы добавлялись тройками: по одному
дополнительному узлу на хост. Как и в предыдущем эксперименте,
преимущество libccpa увеличивается с числом узлов: с примерно трёх
секунд при трёх узлах до 23 секунд при 36 узлах.

Эти результаты ясно показывают, что реализация модели акторы в
libcppa не приводят к потере производительности. А значит
разработчикам не обязательно выбирать между высокоуровневыми
абстракциями и производительностью: эффективная реализация модели
акторов может показывать производительность выше, чем низкоуровнемые
библиотеки.

3. Связанные работы
-------------------

Компоненты многопоточного ПО часто должны иметь общие данные либо
обмениваться данными. В средах с общей памятью этот обмен данных
происходит неявно в виде модификации общего состояния. Тем не менее,
несинхронизированный одновременный доступ к общим сегментам памяти
легко может привести к ошибкам из-за гонок потоков. Для предупреждения
одновременного выполнения (истинно одновременного или чередующегося —
в системах с вытесняющей многозадачностью) кода т.н. *критических
секций* разработчики должны реализовать протоколы сериализации при
помощи низкоуровневых примитивов, таких как мутексы и переменные
синхронизации. Этот подход изначально подвержен ошибкам,
т.к. правильная реализация протокола сериализации требует экспертных
знаний об оптимизациях компилятора, конвейерах процессора и др
[19]. Более того, высоконагруженным компонентам не следует часто
использовать блокировки критических секций, так как они могут легко
стать узким местом производительности. Неблокирующие и незадерживающие
алгоритмы [14] хорошо масштабируются на многопроцессорные системы, но
намного более сложны в реализации [9].

Однако, стратегии блокировок либо неблокирующие алгоритмы — это не
единственная сложность, с которой сталкиваются разработчики в
многопроцессорных системах. Приложения, имеющие хорошую
производительность на однопроцессорной машине могут показывать намного
худшую производительность на многопроцессорной из-за т.н. ложного
шаринга. Ложный шаринг происходит в том случае, когда два или более
процессоров неоднократно производят запись в область памяти,
загруженную в кэш более, чем одного процессора. Это приводит к
взаимной деактуализации кэшей, даже если процессоры обращаются к
различным данным, что значительно замедляет выполнение программы [29].

3.1. Многопоточное программирование и акторы
--------------------------------------------

Для того, чтобы избежать явной синхронизации, были разработаны более
высокоуровневые модели, такие как транзакционная память и модель
обмена сообщениями. Эти парадигмы исключают возникновение гонок
потоков в программых. Так, транзакционная память может быть
реализована аппаратно [15] или программно [24], однако она не
применима ни для событийной модели выполнения программ, ни для
коммуникации компонентов в гетерогенных или распределённых системах. С
другой стороны, модель обмена сообщениями доказала свою
масштабируемость как в многопроцессорных системах, так и в
распределённых системах (таких как кластеры). В области
высокопроизводительных вычислений десятилетиями используются такие
системы обмена сообщениями, как MPI [11].

Модель акторов [16] основывается на парадигме обмена сообщениями, но
поднимает уровень абстракции ещё выше. Она описывает не только
коммуникацию между компонентами системы, но также описывает и сами эти
взаимодействующие компоненты, называемых акторами. В рамках этой
модели параллельные и многопроцессорные системы могут быть составлены
из независимых модулей [1], открытых для взаимодействия с внешними
компонентами [2]. Кроме того, модель акторов обеспечивает надёжную
отказоустойчивое прозрачное сетевое взаимодействие [4]. Таким образом,
разработчикам больше не надо оптимизировать код под особенности
целевых платформ, а развёртывание параллельных гетерогенных
распределённых систем может быть произведена во время выполнения,
т.к. процесс развёртывание не может повлиять на логическую структуру
акторной программы.

3.2. Обмен сообщениями
---------------------

Агха (?) представил модель сообщений по типу почтового ящика в своей
основополагающей статье, посвященной акторам [1]. Почтовый ящик — это
упорядоченный буфер сообщений, организованный по принципу очереди,
доступый на чтение только для хозяина ящика, в то время как все
остальные акторы могут только добавлять сообщения в конец очереди. Так
как акторы не могут иметь общего состояния, всякое взаимодействие
между ними сводится к обмену сообщениями в почтовых ящиках. Сами
почтовые ящики, как правило, реализуются согласно одной из двух
основных концепций.

Согласно первой концепции,  актор последовательно перебирает сообщения
в своём  ящике. Он начинает  с самого первого сообщения,  однако волен
пропустить   любое   сообщение.   Обработка   сообщений   может   быть
автоматически отложена  в случае,  если поведения  актора определяется
частично  заданной функцией  [12].  Так как  акторы  могут менять  своё
поведение в ответ  на сообщение, новое с поведение  может относиться к
ранее пропущенным сообщениям. Сообщение находится  в ящике до тех пор,
пока не  будет обработано,  удаление сообщения выполняется  в процессе
его  обработки.  Классическим  примером акторной  системы  этого  типа
является система сообщений языка Erlang [4].

Вторая концепция предусматривает более строгие правила обработки
сообщений. Обработчик сообщения вызывается ровно один раз для каждого
сообщения, сообщение не может быть пропущено и обработано позже. Т.о.,
акторы обязаны обрабатывать сообщения в порядке их поступления, хотя
некоторые системы позволяют динамически менять обработчик сообщения во
время выполнения. Примерами систем этой категории могут служить SALSA
[31], Akka [30], Kilim [25] и Retlang [23].

В нашей работе мы избрали первый подход. Основными причинами этого
выбора стали возможность обрабатывать сообщения в соответствие с
приоритетами, а также возможность дождаться уточнения запроса перед
тем, как вернуться к поведению по умолчанию. В этой связи
паттерн-матчинг оказался очень полезным и эффективным средством для
определения частичных функций как обработчиков сообщений [3]. Поэтому
в нашей библиотеке мы реализуем маттерн-матчинг как DSL-язык для
обработки сообщений.

3.3. Обработка и распространение ошибок
---------------------------------------

Устойчивые к сбоям распределённые системы требуют мощных механизмов
мониторинга и контроля ошибочных ситуаций. Акторная модель
предписывает акторам наблюдать за состоянием друг друга [16]: в
случае, если какой-либо актор завершается с ошибкой, всем наблюдающим
за ним акторам высылается собщение о его завершении. Два актора могут
каждый могут следить друг за другом, тогда говорят, что эти два актора
*связаны*. Связанные акторы образуют подсистему, в которой происходит
распространение ошибки при помощи сообщений о завершении.

Устанавливая связи между акторами, разработчики могут создать
подсистемы, все акторы которых либо все вместе работают, либо все
вместе завершаются с ошибкой. Можно создать подсистемы, в которых один
или несколько акторов следят за другими актрами и при необходимости
вновь запускают завершившиеся с ошибкой. Развивая мысль в этом
направлении можно построить иерархическую отказоустойчивые системы по
принципу используемых в языке Erlang *деревьев надзора* [4]. Наиболее
современные реализации акторной модели (например, Kilim и Akka)
переняли модель распространения ошибки из Erlang как показывающую
высокую эффективность, элегантность и надёжность [22] в задачах с
высокой связностью. Так как эта модель широко распространена, мы тоже
взяли её в качестве стартовой точки, однако мы стремимся к тому, чтобы
расширить эту модель для того, чтобы она лучше подходила для случаев
слабосвязанных и неиерархических задач.

4. Программная платформа акторного программирования на C++
----------------------------------------------------------

В этом разделе представлена разработанная нами программная платформа,
оформленная в виде написанной на c++ библиотеки libcppa. Ниже будут
освещены её архитектура, а также избранные детали
реализации. Библиотека выпущена под открытой лицензией LGPL 2.1.

4.1. Требования
---------------

К платформе предъявлялись требования как по производительности во
время выполнения, так и по простоте разработки прикладных
программ. Последнее достигается при помощи предоставления разработчику
как можно более высокоуровневых абстракций (но без ущерба для
производительности).

Масштабируемость. В контексте многоядерных процессоров,
мастабируемость требует разбиения логики приложения на много
независимых задач, исполняемых параллельно. Платформа должна позволять
создание значительного числа короткоживущих акторов без ущерба для
производительности. Канонический подход, предполагающий запуск задач в
отдельных потоках, напротив, плохо масштабируется для короткоживущих
задач, так как накладные расходы на создание и уничтожение потока
часто перевешиват выгоду от параллелизации. Следовательно, акторы
должны использовать как можно меньше памяти и иметь быстрый
планировщик задач.

Прозрачная распределённость. Сетевой слой libccpa должен
самостоятельно выполнять всю коммуникацию между узлами, скрывая таким
образом сложность нижележащих протоколов и деплоймента. Более того,
адресация акторов должна полагаться на интерфейс, общий как для
локальных, так и для удалённых акторов, обеспечивая приложению
единообразный прозрачный доступ и простое перемасштабирование во время
выполнения.

Обработка сообщений. Сообщения акторов должны (а) удаляться сборщиком
мусора, (б) быть произвольными типами данных, (в) быть доступными для
паттерн-матчинга. Требование (а) вытекает из эмпирического знания о
том, что ручное управление памятью в параллельных системах чревато
ошибками и таким образом непрактично, в то время как альтернативный
подход — копировать сообщения отдельно для каждого получателя — ударит
по производительности (субоптимальный) для сообщений, направленных
многим получателям. Требование (б) отражает общий опыт, согласно
которому системы обмена сообщениями с ограниченной системой типов
имеют ограниченное использование на практике. Тем не менее, для снятия
ограничения на типы сообщения необходимо наличие эффективных и
выразительных средств их обработки, таких, как паттерн-матчинг,
так-как обработка сообщения — очень часто встречающая операция (в
акторном программировании).

4.2. Ключевые понятия архитектуры и реализации
----------------------------------------------

В этом разделе представлены избранные детали важных с технологической
и алгоритмической точке зрения проектных решений, призванных
обеспечить выполнение требований и предоставить разработчику
современный API на C++.

4.2.1. Кортежи и отложенное копирование
--------------------------------------

Реализация системы сообщений libccpa использует кортежи с семантикой
типов-значений, что приводит к множественному копированию кортежа при
отправке сообщения более чем одному актору. Поэтому используется
техника оптимизации, известная как отложенное копирование, которая
заключается в том, что несколько акторов могут совместно использовать
один и тот же экземпляр кортежа до тех пор, пока используют его только
для чтения. Если какой-либо из этих акторов пытается выполнить запись,
производится фактическое копирование кортежа и далее актор работает с
собственной копией. Таким образом и исключается состояние гонки, так
как производится копирование разделяемого объекта, и само копирование
происходит только при необходиомсти. Этот механизм также позволяет
осуществлять сборку мусора, так как кортежи, на которые не ссылается
ни один актор, могут быть удалены автоматически. В библиотеке
использована атомарная интрузивная (?) реализация подсчёта ссылок,
влияние которой на производительность пренебрежимо мало.

4.2.2. Алгоритм почтового ящика
------------------------------

Реализация очереди сообщений, или *почтового ящика*, — критически
важный компонент любой системы обмена сообщениями. Все сообщения,
отправляемые актору, доставляются в его почтовый ящик, что подобно
доступу к общему ресурсу в случае, когда актор одновременно получает
множество сообщений от разных отправителей. Таким образом, общая
производительность системы, а особенно её способность к
масштабируемости, значительным образом зависит от выбранного
алгоритма.

Почтовый ящик — это очередь с единственным читателем и множеством
писателей. Он предоставляет параллельный доступ на запись, но только
актор-владелец забирать элементы из очереди. Следовательно, операция
снятия элемента из очереди не обязана поддерживать параллельный
доступ. Мы объединили неблокирующую реализацию стека и строго
упорядоченную очередь, используемую в качестве внутреннего
кэша.
